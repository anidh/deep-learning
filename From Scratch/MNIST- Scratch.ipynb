{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing our mnist dataset\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing\n",
    "X=X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the X is (70000, 784) and the shape of y is (70000,)\n"
     ]
    }
   ],
   "source": [
    "#Printing the shape of the both\n",
    "print(\"The shape of the X is {} and the shape of y is {}\".format(X.shape,y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a zero number classifier here\n",
    "import numpy as np\n",
    "\n",
    "y_new = np.zeros(y.shape)\n",
    "y_new[np.where(y == 0.0)[0]] = 1\n",
    "y = y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now building the train and test sets\n",
    "m=X.shape[0]-10000\n",
    "m_test=X.shape[0]-m\n",
    "X_train,X_test=X[:m].T,X[m:].T\n",
    "y_train,y_test=(y[:m].reshape(1,m)),(y[m:].reshape(1,m_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shapes of X_train and X_test are (784, 60000) (784, 10000)\n",
      "The Shapes of y_train and y_test are (1, 60000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "#New SHapes are-\n",
    "print(\"The Shapes of X_train and X_test are {} {}\".format(X_train.shape,X_test.shape))\n",
    "print(\"The Shapes of y_train and y_test are {} {}\".format(y_train.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the datasets here\n",
    "np.random.seed(138) # consistent examples\n",
    "shuffle_index = np.random.permutation(m) #The random number\n",
    "X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index] #shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "#Looking at an example image\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 3\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(y_train[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we observe that the input X is of the form 784x70000 which means that 784 we get from 28*28 and it's an greyscaale image\n",
    "# next 70000 images are there.\n",
    "#Now implementing the sigmoid function which will be used\n",
    "def sigmoid(z):\n",
    "    s= (1/(1+(np.exp(-z))))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will define our cost function here\n",
    "def compute_loss(Y, Y_hat):\n",
    "    m = Y.shape[1]\n",
    "    #the cross entropy loss\n",
    "    L = -(1./m) * ( np.sum( np.multiply(np.log(Y_hat),Y) ) +\n",
    "                   np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0 and Cost is 0.684080159543643\n",
      "Epoch is 100 and Cost is 0.041305162058342754\n",
      "Epoch is 200 and Cost is 0.03578100961598483\n",
      "Epoch is 300 and Cost is 0.03321767707346867\n",
      "Epoch is 400 and Cost is 0.031618250521307165\n",
      "Epoch is 500 and Cost is 0.03047987992166962\n",
      "Epoch is 600 and Cost is 0.029609158581624707\n",
      "Epoch is 700 and Cost is 0.028912781392366863\n",
      "Epoch is 800 and Cost is 0.028338723798575546\n",
      "Epoch is 900 and Cost is 0.027854953682088075\n",
      "Epoch is 1000 and Cost is 0.02744030579355481\n",
      "Epoch is 1100 and Cost is 0.027080051475541836\n",
      "Epoch is 1200 and Cost is 0.026763531573280784\n",
      "Epoch is 1300 and Cost is 0.02648279278126125\n",
      "Epoch is 1400 and Cost is 0.026231753648863553\n",
      "Epoch is 1500 and Cost is 0.026005669485284065\n",
      "Epoch is 1600 and Cost is 0.02580077544360537\n",
      "Epoch is 1700 and Cost is 0.02561404072768779\n",
      "Epoch is 1800 and Cost is 0.02544299474890846\n",
      "Epoch is 1900 and Cost is 0.025285601356506867\n",
      "The Final Cost is 0.025141566084818247\n"
     ]
    }
   ],
   "source": [
    "# Now defining our model\n",
    "learning_rate=1\n",
    "X=X_train\n",
    "Y=y_train\n",
    "n_x=X.shape[0]\n",
    "m=X.shape[1]\n",
    "W=np.random.randn(n_x,1)*0.01\n",
    "b=np.zeros((1,1))\n",
    "for i in range(2000):\n",
    "    #The Forward Propagation Step\n",
    "    Z=np.matmul(W.T,X)+b\n",
    "    A=sigmoid(Z)\n",
    "    cost=compute_loss(Y,A)\n",
    "    #The Backpropagation Step\n",
    "    dW=(1/m)*np.matmul(X,(A-Y).T)\n",
    "    db=(1/m)*np.sum(A-Y,axis=1,keepdims=True)\n",
    "    #Updation Rule\n",
    "    W=W-learning_rate*dW\n",
    "    b=b-learning_rate*db\n",
    "    if (i%100==0):\n",
    "        print(\"Epoch is {} and Cost is {}\".format(i,cost))\n",
    "print(\"The Final Cost is {}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8980   33]\n",
      " [  40  947]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "Z = np.matmul(W.T, X_test) + b\n",
    "A = sigmoid(Z)\n",
    "\n",
    "predictions = (A >.5)[0,:]\n",
    "labels = (y_test == 1)[0,:]\n",
    "\n",
    "print(confusion_matrix(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00      9013\n",
      "       True       0.97      0.96      0.96       987\n",
      "\n",
      "avg / total       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hiddden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0 and Cost is 0.9144384083567224\n",
      "Epoch is 100 and Cost is 0.11617827938780523\n",
      "Epoch is 200 and Cost is 0.08497301210857518\n",
      "Epoch is 300 and Cost is 0.07235373202856842\n",
      "Epoch is 400 and Cost is 0.06483700276428704\n",
      "Epoch is 500 and Cost is 0.05958019324382801\n",
      "Epoch is 600 and Cost is 0.05554178302597507\n",
      "Epoch is 700 and Cost is 0.05223959677460944\n",
      "Epoch is 800 and Cost is 0.049474452643074\n",
      "Epoch is 900 and Cost is 0.047125882776230664\n",
      "Epoch is 1000 and Cost is 0.04509970632851582\n",
      "Epoch is 1100 and Cost is 0.04334187296072376\n",
      "Epoch is 1200 and Cost is 0.04181093092065309\n",
      "Epoch is 1300 and Cost is 0.04046592785655237\n",
      "Epoch is 1400 and Cost is 0.03926893165824677\n",
      "Epoch is 1500 and Cost is 0.03819026583616659\n",
      "Epoch is 1600 and Cost is 0.03720929593349891\n",
      "Epoch is 1700 and Cost is 0.03631185660439528\n",
      "Epoch is 1800 and Cost is 0.03548729462063055\n",
      "Epoch is 1900 and Cost is 0.03472673547376728\n",
      "The Final Cost is 0.03402927957320837\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1\n",
    "\n",
    "n_h=64\n",
    "n_x=X.shape[0]\n",
    "\n",
    "W1=np.random.randn(n_h,n_x)\n",
    "W2=np.random.randn(1,n_h)\n",
    "b1=np.zeros((n_h,1))\n",
    "b2=np.zeros((1,1))\n",
    "\n",
    "X=X_train\n",
    "y=y_train\n",
    "m=X.shape[1]\n",
    "for i in range(2000):\n",
    "    #forward propagation\n",
    "    Z1=np.matmul(W1,X)+b1\n",
    "    A1=sigmoid(Z1)\n",
    "    Z2=np.matmul(W2,A1)+b2\n",
    "    A2=sigmoid(Z2)\n",
    "\n",
    "    #Computing The Loss\n",
    "    cost=compute_loss(Y,A2)\n",
    "    \n",
    "    #Now Backpropagation Example\n",
    "    dZ2=A2-Y\n",
    "    dW2=(1/m)*np.matmul(dZ2,A2.T)\n",
    "    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    #-----\n",
    "    dZ1=np.matmul(W2.T,dZ2)*sigmoid(Z1)*(1-sigmoid(Z1))\n",
    "    dW1=(1/m)*np.matmul(dZ1,X.T)\n",
    "    db1=(1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    #Updation Rule\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    \n",
    "    #Printing Epochs\n",
    "    if (i%100 == 0):\n",
    "        print(\"Epoch is {} and Cost is {}\".format(i,cost))\n",
    "print(\"The Final Cost is {}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8974   54]\n",
      " [  46  926]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99      9028\n",
      "       True       0.94      0.95      0.95       972\n",
      "\n",
      "avg / total       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.matmul(W1, X_test) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "predictions = (A2>.5)[0,:]\n",
    "labels = (y_test == 1)[0,:]\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and standardinz\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape before (1, 70000, 10)\n",
      "the shape after (10, 70000)\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding the labels\n",
    "digits=10\n",
    "examples=y.shape[0]\n",
    "y=y.reshape(1,examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "print(\"the shape before {}\".format(Y_new.shape))\n",
    "Y_new=Y_new.T.reshape(digits,examples)\n",
    "print(\"the shape after {}\".format(Y_new.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshuffling and splitting\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n",
    "\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shapes of X_train and X_test are (784, 60000) (784, 10000)\n",
      "The Shapes of y_train and y_test are (1, 60000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Shapes of X_train and X_test are {} {}\".format(X_train.shape,X_test.shape))\n",
    "print(\"The Shapes of y_train and y_test are {} {}\".format(y_train.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABYBJREFUeJzt3bFL1H8cx/GfP2xVzKFc+gcCJXJwFEKHJKi1xVVnFyeDlhCn6C9wC6HFRUMaFGwJkv6BcGhpsNRGFfsLvu+Tu+7Uez0e68v73i1PvsPH793A5eXlf0Ce/6/7AwDXQ/wQSvwQSvwQSvwQSvwQSvwQSvwQSvwQarDH7+ffCaH7Bq7yR+78EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EKrXP9ENV3ZxcVHuT58+Lffv3783bnt7e+Vrx8bGyr0fuPNDKPFDKPFDKPFDKPFDKPFDKPFDKOf83FifP38u952dnbav/fPnz3J3zg/0LfFDKPFDKPFDKPFDKPFDKPFDKOf8XJtWZ+3z8/MdXX9iYqJxe/DgQUfX7gfu/BBK/BBK/BBK/BBK/BBK/BDKUR/XZn19vdwPDw87uv6jR48at7t373Z07X7gzg+hxA+hxA+hxA+hxA+hxA+hxA+hBi4vL3v5fj19M67ft2/fGrdnz56Vr/3x40e5Dw8Pl/vu7m7jNj4+Xr72lhu4yh+580Mo8UMo8UMo8UMo8UMo8UMo8UMoz/PTkfPz83J/9epV49bqHL+VtbW1cu/zs/yOufNDKPFDKPFDKPFDKPFDKPFDKPFDKOf8dOTg4KDcNzc32772yMhIuU9OTrZ9bdz5IZb4IZT4IZT4IZT4IZT4IZT4IZTv7acjrX7n/vj4uHFr9b37Gxsb5T4zM1PuwXxvP9BM/BBK/BBK/BBK/BBK/BDKI72U3r59W+6/f/8u94GB5lOne/fula91lNdd7vwQSvwQSvwQSvwQSvwQSvwQSvwQyiO94b58+VLuT548Kfc/f/6U+9DQUOP28ePH8rVTU1PlTiOP9ALNxA+hxA+hxA+hxA+hxA+hxA+hPM/f587Ozsr9zZs35d7qHL+V1dXVxs05/vVy54dQ4odQ4odQ4odQ4odQ4odQ4odQnufvcy9fviz39+/fd3T9Fy9elPuHDx86uj5t8Tw/0Ez8EEr8EEr8EEr8EEr8EEr8EMo5fx+ovnt/dna2fO3JyUm5j46OlvunT5/KfXx8vNzpCuf8QDPxQyjxQyjxQyjxQyjxQyhf3X0LnJ6elnv19dutjvJaWVlZKXdHebeXOz+EEj+EEj+EEj+EEj+EEj+EEj+E8kjvLbC1tVXuc3NzbV/7+fPn5b6xsVHug4P+VeQG8kgv0Ez8EEr8EEr8EEr8EEr8EEr8EMoh7Q3w69evcn/37l3X3nt6errcneP3L3d+CCV+CCV+CCV+CCV+CCV+CCV+COUQ9wZYWloq9+3t7bavvby8XO6Li4ttX5vbzZ0fQokfQokfQokfQokfQokfQjnq64Gjo6Ny//r1a9fee3h4uNzPz8/L/c6dO//y43CDuPNDKPFDKPFDKPFDKPFDKPFDKPFDKD/R3QMHBwfl/vjx466998OHD8t9f3+/3IeGhv7lx6E3/EQ30Ez8EEr8EEr8EEr8EEr8EEr8EMo5fw9cXFyU+8LCQrnv7OyU++zsbOP2+vXr8rX3798vd24l5/xAM/FDKPFDKPFDKPFDKPFDKPFDKOf80H+c8wPNxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hxA+hBnv8flf66WCg+9z5IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IZT4IdRfQ8S7a4suehIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying an random image\n",
    "i = 12\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "Y_train[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiclass loss is\n",
    "def compute_multiclass_loss(Y,Y_hat):\n",
    "    L_sum=np.sum(np.multiply(Y,np.log(Y_hat)))\n",
    "    m=Y.shape[1]\n",
    "    L=-(1/m)*L_sum\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epoch 0 the loss is 2.303426641981376\n",
      "For Epoch 100 the loss is 0.5282148573724671\n",
      "For Epoch 200 the loss is 0.3521313707999565\n",
      "For Epoch 300 the loss is 0.3002757054503863\n",
      "For Epoch 400 the loss is 0.27042133118105294\n",
      "For Epoch 500 the loss is 0.24824994537017017\n",
      "For Epoch 600 the loss is 0.23022704494553597\n",
      "For Epoch 700 the loss is 0.21506717979750667\n",
      "For Epoch 800 the loss is 0.20204021803040603\n",
      "For Epoch 900 the loss is 0.19066879979380966\n",
      "For Epoch 1000 the loss is 0.180630994544226\n",
      "For Epoch 1100 the loss is 0.17169318482968537\n",
      "For Epoch 1200 the loss is 0.16367440466113203\n",
      "For Epoch 1300 the loss is 0.15643008554571602\n",
      "For Epoch 1400 the loss is 0.14984326094995878\n",
      "For Epoch 1500 the loss is 0.14381873828971148\n",
      "For Epoch 1600 the loss is 0.13827881328114466\n",
      "For Epoch 1700 the loss is 0.1331599793140135\n",
      "For Epoch 1800 the loss is 0.12841028231112092\n",
      "For Epoch 1900 the loss is 0.12398706770865955\n",
      "The final cost is 0.1198950255596623\n"
     ]
    }
   ],
   "source": [
    "#Building our model\n",
    "n_x=X_train.shape[0]\n",
    "n_h=64\n",
    "learning_rate=1\n",
    "\n",
    "W1=np.random.randn(n_h,n_x)*0.01\n",
    "W2=np.random.randn(digits,n_h)*0.01\n",
    "b1=np.zeros((n_h,1))\n",
    "b2=np.zeros((digits,1))\n",
    "\n",
    "X=X_train\n",
    "Y=Y_train\n",
    "\n",
    "#---Forward Propagation----#\n",
    "\n",
    "for i in range(2000):\n",
    "    Z1=np.matmul(W1,X)+b1\n",
    "    A1=sigmoid(Z1)\n",
    "    Z2=np.matmul(W2,A1)+b2\n",
    "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0) #SOFTMAX FUNC\n",
    "    \n",
    "#---The Loss----#    \n",
    "    \n",
    "    cost=compute_multiclass_loss(Y,A2)\n",
    "\n",
    "#---Backward Propagation----#\n",
    "    \n",
    "    dZ2=A2-Y\n",
    "    dW2=(1/m)*np.matmul(dZ2,A1.T)\n",
    "    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    dZ1=np.matmul(W2.T,dZ2)* sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "    dW1=(1/m)*np.matmul(dZ1,X.T)\n",
    "    db1=(1/m)*np.sum(dZ1)\n",
    "\n",
    "#---Updation Rule----#\n",
    "\n",
    "    W1=W1-learning_rate*dW1\n",
    "    b1=b1-learning_rate*db1\n",
    "    W2=W2-learning_rate*dW2\n",
    "    b2=b2-learning_rate*db2\n",
    "    \n",
    "    if (i%100 == 0):\n",
    "        print(\"For Epoch {} the loss is {}\".format(i,cost))\n",
    "print(\"The final cost is {}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 966    0    6    2    1    7    7    1    5    8]\n",
      " [   0 1117    1    1    0    1    3    8    3    7]\n",
      " [   1    2  994    5    5    1    0   16    3    1]\n",
      " [   2    2    6  977    1   18    1    5   13   10]\n",
      " [   0    0    6    0  938    2    5    3    4   22]\n",
      " [   4    1    1    5    0  839    8    1    5    5]\n",
      " [   4    5    6    1    5   11  929    0    6    0]\n",
      " [   2    2    5    9    3    2    0  981    5    9]\n",
      " [   1    6    5    7    3    6    5    0  927    4]\n",
      " [   0    0    2    3   26    5    0   13    3  943]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.97      1003\n",
      "          1       0.98      0.98      0.98      1141\n",
      "          2       0.96      0.97      0.97      1028\n",
      "          3       0.97      0.94      0.96      1035\n",
      "          4       0.96      0.96      0.96       980\n",
      "          5       0.94      0.97      0.95       869\n",
      "          6       0.97      0.96      0.97       967\n",
      "          7       0.95      0.96      0.96      1018\n",
      "          8       0.95      0.96      0.96       964\n",
      "          9       0.93      0.95      0.94       995\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.matmul(W1, X_test) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "\n",
    "predictions = np.argmax(A2, axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimization, Xavier Initialization & Mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# import\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "#Scaling our data\n",
    "X=X/255\n",
    "\n",
    "# one-hot encode labels\n",
    "digits = 10\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "# split, reshape, shuffle\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key functions used in our model\n",
    "#sigmoid func\n",
    "def sigmoid(z):\n",
    "    exp=(1/(1+(np.exp(-z))))\n",
    "    return exp\n",
    "\n",
    "#the second step which is to forward propagation step\n",
    "def feed_forward(X,params):\n",
    "    cache={}\n",
    "    cache[\"Z1\"]=np.matmul(params[\"W1\"],X)+params[\"b1\"]\n",
    "    cache[\"A1\"]=np.tanh(cache[\"Z1\"])\n",
    "    cache[\"Z2\"]=np.matmul(params[\"W2\"],cache[\"A1\"])+params[\"b2\"]\n",
    "    cache[\"A2\"]=(np.exp(cache[\"Z2\"]))/(np.sum(np.exp(cache[\"Z2\"]),axis=0))\n",
    "    return cache\n",
    "    \n",
    "#The third step  which is to compute loss\n",
    "def compute_loss(Y,Y_hat):\n",
    "    L_sum=np.sum(np.multiply(Y,np.log(Y_hat)))\n",
    "    m=Y.shape[1]\n",
    "    L=-(1/m)*L_sum\n",
    "    return L\n",
    "\n",
    "#The fourth step which is to propagate backward\n",
    "def back_propagate(X,Y,params,cache):\n",
    "    dZ2=cache[\"A2\"]-Y\n",
    "    dW2=(1/m_batch)*np.matmul(dZ2,cache[\"A1\"].T)\n",
    "    db2=(1/m_batch)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1=np.matmul(params[\"W2\"].T,dZ2)*(1-np.power(cache[\"A1\"], 2)) #sigmoid(cache[\"Z1\"])*(1-sigmoid(cache[\"Z1\"]))\n",
    "    dW1=(1/m_batch)*np.matmul(dZ1,X.T)\n",
    "    db1=(1/m_batch)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    grads={\"dW1\":dW1,\"db1\":db1,\"dW2\":dW2,\"db2\":db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of mini batches which we got are 235\n",
      "Epoch 10: training cost = 0.029217542212520493, test cost = 0.09736250610975704 and the time it took is 2.5039970874786377\n",
      "Epoch 20: training cost = 0.017776507624177717, test cost = 0.12080741972351196 and the time it took is 2.758000373840332\n",
      "Epoch 30: training cost = 0.0015995171871713639, test cost = 0.1231315080923304 and the time it took is 2.50799822807312\n",
      "Epoch 40: training cost = 0.0010291417324252154, test cost = 0.12583635807741242 and the time it took is 1.9799995422363281\n",
      "Epoch 50: training cost = 0.0004336551596294795, test cost = 0.13682282665580592 and the time it took is 2.0130040645599365\n",
      "Epoch 60: training cost = 0.0023401754729948866, test cost = 0.1492089466249209 and the time it took is 1.8659989833831787\n",
      "Epoch 70: training cost = 0.00035552383260366804, test cost = 0.14616949824047581 and the time it took is 2.085000514984131\n",
      "Epoch 80: training cost = 0.0001618609175651977, test cost = 0.15530065948438435 and the time it took is 2.077997922897339\n",
      "Epoch 90: training cost = 0.0008471924549042426, test cost = 0.1532492546120021 and the time it took is 2.4049980640411377\n",
      "Epoch 100: training cost = 0.000302821589296739, test cost = 0.1552754851233464 and the time it took is 2.0920028686523438\n",
      "------------------------------------------Done------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#building and training our model\n",
    "np.random.seed(138)\n",
    "\n",
    "#setting the hyperparameters here\n",
    "learning_rate=0.01\n",
    "beta1=0.9\n",
    "beta2=0.999\n",
    "epsilon=1e-8\n",
    "n_x=X_train.shape[0]\n",
    "n_h=64\n",
    "batch_size=256\n",
    "batches = -(-m // batch_size)\n",
    "\n",
    "\n",
    "#Now initializing all the params\n",
    "params={\"W1\":np.random.randn(n_h,n_x)*np.sqrt(1/(n_x)),\n",
    "        \"W2\":np.random.randn(digits,n_h)*np.sqrt(1/(n_h)),\n",
    "        \"b1\":np.zeros((n_h,1))*np.sqrt(1/(n_x)),\n",
    "        \"b2\":np.zeros((digits,1))*np.sqrt(1/(n_h))\n",
    "       }\n",
    "\n",
    "V_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "V_db1 = np.zeros(params[\"b1\"].shape)\n",
    "V_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "V_db2 = np.zeros(params[\"b2\"].shape)\n",
    "#For Adam Optimization\n",
    "S_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "S_db1 = np.zeros(params[\"b1\"].shape)\n",
    "S_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "S_db2 = np.zeros(params[\"b2\"].shape)\n",
    "\n",
    "# Prinitng the total number of batches we got\n",
    "print(\"The total number of mini batches which we got are {}\".format(batches))\n",
    "\n",
    "#Training Our Model Here\n",
    "for i in range(1,101): #Our epochs\n",
    "    import time\n",
    "    start = time.time()\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "    for j in range(1,batches):\n",
    "        #Traversing Through Mini Batches\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "        #1 pass through gd ~ no cost here\n",
    "        cache=feed_forward(X,params)\n",
    "        grads=back_propagate(X,Y,params,cache)\n",
    "        #momentum\n",
    "        V_dW1=(beta1*V_dW1+(1-beta1)*grads[\"dW1\"])\n",
    "        V_db1=(beta1*V_db1+(1-beta1)*grads[\"db1\"])\n",
    "        V_dW2=(beta1*V_dW2+(1-beta1)*grads[\"dW2\"])\n",
    "        V_db2=(beta1*V_db2+(1-beta1)*grads[\"db2\"])\n",
    "        #For AdaM OPTIMIZATION\n",
    "        S_dW1=(beta2*S_dW1+(1-beta2)*(grads[\"dW1\"]*grads[\"dW1\"]))\n",
    "        S_db1=(beta2*S_db1+(1-beta2)*(grads[\"db1\"]*grads[\"db1\"]))\n",
    "        S_dW2=(beta2*S_dW2+(1-beta2)*(grads[\"dW2\"]*grads[\"dW2\"]))\n",
    "        S_db2=(beta2*S_db2+(1-beta2)*(grads[\"db2\"]*grads[\"db2\"]))\n",
    "        #All the corrected terms\n",
    "        V_dW1_Corr=(V_dW1)/(1-(beta1**j))\n",
    "        V_dW2_Corr=(V_dW2)/(1-(beta1**j))\n",
    "        V_db1_Corr=(V_db1)/(1-(beta1**j))\n",
    "        V_db2_Corr=(V_db2)/(1-(beta1**j))\n",
    "        #RMSProp\n",
    "        S_dW1_Corr=(S_dW1)/(1-(beta2**j))\n",
    "        S_dW2_Corr=(S_dW2)/(1-(beta2**j))\n",
    "        S_db1_Corr=(S_db1)/(1-(beta2**j))\n",
    "        S_db2_Corr=(S_db2)/(1-(beta2**j))\n",
    "        #updation rule\n",
    "        params[\"W1\"]=params[\"W1\"]-learning_rate*(V_dW1_Corr/np.sqrt(S_dW1_Corr + epsilon))\n",
    "        params[\"b1\"]=params[\"b1\"]-learning_rate*(V_db1_Corr/np.sqrt(S_db1_Corr + epsilon))\n",
    "        params[\"W2\"]=params[\"W2\"]-learning_rate*(V_dW2_Corr/np.sqrt(S_dW2_Corr + epsilon))\n",
    "        params[\"b2\"]=params[\"b2\"]-learning_rate*(V_db2_Corr/np.sqrt(S_db2_Corr + epsilon))\n",
    "    \n",
    "    #Training Here\n",
    "    cache = feed_forward(X_train, params)\n",
    "    train_cost=compute_loss(Y_train, cache[\"A2\"])\n",
    "    #testing here\n",
    "    cache = feed_forward(X_test, params)\n",
    "    test_cost = compute_loss(Y_test, cache[\"A2\"])\n",
    "    if(i % 10 == 0):\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        print(\"Epoch {}: training cost = {}, test cost = {} and the time it took is {}\".format(i ,train_cost, test_cost,elapsed))\n",
    "\n",
    "print(\"------------------------------------------Done------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       987\n",
      "          1       0.99      0.99      0.99      1142\n",
      "          2       0.98      0.97      0.97      1042\n",
      "          3       0.97      0.97      0.97      1012\n",
      "          4       0.97      0.97      0.97       980\n",
      "          5       0.96      0.96      0.96       891\n",
      "          6       0.98      0.98      0.98       958\n",
      "          7       0.96      0.97      0.97      1023\n",
      "          8       0.96      0.97      0.97       961\n",
      "          9       0.96      0.96      0.96      1004\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache = feed_forward(X_test, params)\n",
    "predictions = np.argmax(cache[\"A2\"], axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
