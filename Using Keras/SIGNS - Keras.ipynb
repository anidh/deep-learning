{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorflow version is 1.12.0\n"
     ]
    }
   ],
   "source": [
    "#SIGNS Dataset using python\n",
    "#importing the various libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense,Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "import cv2\n",
    "import math\n",
    "import h5py\n",
    "import time\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib inline\n",
    "print(\"The tensorflow version is \"+str(tf.__version__))\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset using pickle\n",
    "def load_dataset():\n",
    "     # Loading the data (cat/non-cat)\n",
    "    train_dataset = h5py.File('train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # train set labels\n",
    "\n",
    "    test_dataset = h5py.File('test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # test set labels\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-x-x-x-x-x-x-x--x-x-x-x-x-x-x-x-Printing The Shapes-x-x-x-x-x-x-x-x-x-x-x-x-x--x\n",
      "The Training Shapes are for X (1080, 64, 64, 3) and for Y (1080,) \n",
      "The Testing Shapes are for X (120, 64, 64, 3) and for Y (120,) \n"
     ]
    }
   ],
   "source": [
    "#loading the dataset\n",
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=load_dataset()\n",
    "print(\"-x-x-x-x-x-x-x--x-x-x-x-x-x-x-x-Printing The Shapes-x-x-x-x-x-x-x-x-x-x-x-x-x--x\")\n",
    "print(\"The Training Shapes are for X {} and for Y {} \".format(train_set_x_orig.shape,train_set_y_orig.shape))\n",
    "print(\"The Testing Shapes are for X {} and for Y {} \".format(test_set_x_orig.shape,test_set_y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New Shapes are---\n",
      "X_train shape is (1080, 12288) and Y_train shape is  (1080, 6)\n",
      "X_test shape is (120, 12288) and Y_test shape is  (120, 6)\n"
     ]
    }
   ],
   "source": [
    "#Reshaping them here\n",
    "X_train=train_set_x_orig.reshape((train_set_x_orig.shape[0]),(train_set_x_orig.shape[1]*train_set_x_orig.shape[2]*train_set_x_orig.shape[3]))\n",
    "X_test=test_set_x_orig.reshape((test_set_x_orig.shape[0]),(train_set_x_orig.shape[1]*train_set_x_orig.shape[2]*train_set_x_orig.shape[3]))\n",
    "#One hot encoding the labels\n",
    "lb=LabelBinarizer()\n",
    "Y_train=lb.fit_transform(train_set_y_orig)\n",
    "Y_test=lb.transform(test_set_y_orig)\n",
    "print(\"The New Shapes are---\")\n",
    "print(\"X_train shape is {} and Y_train shape is  {}\".format(X_train.shape,Y_train.shape))\n",
    "print(\"X_test shape is {} and Y_test shape is  {}\".format(X_test.shape,Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizng and shuffling the indices\n",
    "X_train=X_train/255\n",
    "X_test=X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 972 samples, validate on 108 samples\n",
      "Epoch 1/300\n"
     ]
    }
   ],
   "source": [
    "lr=[0.0003]\n",
    "dropouts=[0.25]\n",
    "for learning_rate in lr:\n",
    "    for dropout in dropouts:\n",
    "        model = Sequential()\n",
    "        NAME=\"Model With {} dropout and {} layers at {} time @ learning rate of {}\".format(dropout,128,int(time.time()),learning_rate)\n",
    "        tensorboard=TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation=tf.nn.relu))\n",
    "        model.add(Dense(128,activation=tf.nn.relu))\n",
    "        model.add(Dense(128,activation=tf.nn.relu))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(128,activation=tf.nn.relu))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(128,activation=tf.nn.relu))\n",
    "        model.add(Dense(6,activation=tf.nn.softmax))\n",
    "        adamop=keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(optimizer=\"adam\",metrics=[\"accuracy\"],loss=\"categorical_crossentropy\")\n",
    "        #install hdf5 package before proceeding ~pip install \n",
    "        #filepath=\"weights.best.hdf5\"\n",
    "        #checkpoint=ModelCheckpoint(filepath,monitor=\"val_acc\",verbose=1,save_best_only=True,mode=\"max\")\n",
    "        callback_list=[tensorboard]\n",
    "        model.fit(X_train,Y_train,shuffle=True,validation_split=0.1,batch_size=32,epochs=300,callbacks=callback_list)\n",
    "        test_loss,test_acc=model.evaluate(X_test,Y_test)\n",
    "        print(\"******** Testing Loss {} and Accuracy {} ****************\".format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
