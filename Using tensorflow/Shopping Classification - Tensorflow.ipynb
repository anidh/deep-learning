{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version -->1.12.0\n"
     ]
    }
   ],
   "source": [
    "#importing all the necessary libraraies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"Tensorflow version -->\" +str(tf.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the original shapes\n",
      "For trainng X (60000, 28, 28) and Y is  (60000,)\n",
      "For testing X (10000, 28, 28) and Y is (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset and preproceesing it\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_orig, Y_train), (x_test_orig, Y_test) = fashion_mnist.load_data()\n",
    "#Now standardizing and sclaing the images\n",
    "print(\"Printing the original shapes\")\n",
    "print(\"For trainng X {} and Y is  {}\".format(x_train_orig.shape,Y_train.shape))\n",
    "print(\"For testing X {} and Y is {}\".format(x_test_orig.shape,Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_train (60000, 784) and X_test (10000, 784)\n",
      "One hot encoding results in Y_train (60000, 10) and Y_test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Resizing and one hot encoding the images\n",
    "X_train=x_train_orig.reshape(x_train_orig.shape[0],x_train_orig.shape[1]*x_train_orig.shape[2])\n",
    "X_test=x_test_orig.reshape(x_test_orig.shape[0],x_test_orig.shape[1]*x_test_orig.shape[2])\n",
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "lb=LabelBinarizer()\n",
    "Y_train=lb.fit_transform(Y_train)\n",
    "Y_test=lb.transform(Y_test)\n",
    "print(\"The X_train {} and X_test {}\".format(X_train.shape,X_test.shape))\n",
    "print(\"One hot encoding results in Y_train {} and Y_test {}\".format(Y_train.shape,Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling and Randomizing batches\n",
    "def random_mini_batches(X, Y,seed, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    #The Code is Transformed for m,n_x Case#\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:]\n",
    "    shuffled_Y = Y[permutation,:].reshape((m,Y.shape[1]))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designing The placeholders now\n",
    "def placeholder(n_x,n_y,keep_prob):\n",
    "    X=tf.placeholder(tf.float32,[None,n_x],name=\"X\")\n",
    "    Y=tf.placeholder(tf.float32,[None,n_y],name=\"Y\")\n",
    "    keep_prob=tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    return X,Y,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = {}Tensor(\"X:0\", shape=(?, 784), dtype=float32)\n",
      "Y = {}Tensor(\"Y:0\", shape=(?, 10), dtype=float32)\n",
      "Keep Probability = {}Tensor(\"keep_prob:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X,Y,keep_prob=placeholder(784,10,1)\n",
    "print(\"X = {}\"+str(X))\n",
    "print(\"Y = {}\"+str(Y))\n",
    "print(\"Keep Probability = {}\"+str(keep_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the weights\n",
    "def initialize_weights():\n",
    "    W1=tf.get_variable(\"W1\",[784,128],initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1=tf.get_variable(\"b1\",[128],initializer=tf.zeros_initializer())\n",
    "    W2=tf.get_variable(\"W2\",[128,256],initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2=tf.get_variable(\"b2\",[256],initializer=tf.zeros_initializer())\n",
    "    W3=tf.get_variable(\"W3\",[256,128],initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3=tf.get_variable(\"b3\",[128],initializer=tf.zeros_initializer())\n",
    "    W4=tf.get_variable(\"W4\",[128,10],initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b4=tf.get_variable(\"b4\",[10],initializer=tf.zeros_initializer())\n",
    "    \n",
    "    parameters={\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2,\"W3\":W3,\"b3\":b3,\"W4\":W4,\"b4\":b4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = {}<tf.Variable 'W1:0' shape=(784, 128) dtype=float32_ref>\n",
      "b1 = {}<tf.Variable 'b1:0' shape=(128,) dtype=float32_ref>\n",
      "------------------------------\n",
      "b4 = {}<tf.Variable 'W4:0' shape=(128, 10) dtype=float32_ref>\n",
      "b4 = {}<tf.Variable 'b4:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X,Y,keep_prob=placeholder(784,10,1)\n",
    "parameters=initialize_weights()\n",
    "print(\"W1 = {}\" +str(parameters[\"W1\"]))\n",
    "print(\"b1 = {}\" +str(parameters[\"b1\"]))\n",
    "print(\"------------------------------\")\n",
    "print(\"b4 = {}\" +str(parameters[\"W4\"]))\n",
    "print(\"b4 = {}\" +str(parameters[\"b4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Doing the feed forward \n",
    "def feed_forward(X,parameters,keep_prob):\n",
    "    Z1=tf.add(tf.matmul(X,parameters[\"W1\"]),parameters[\"b1\"])\n",
    "    A1=tf.nn.relu(Z1)\n",
    "    A1d=tf.nn.dropout(A1,keep_prob)\n",
    "    Z2=tf.add(tf.matmul(A1,parameters[\"W2\"]),parameters[\"b2\"])\n",
    "    A2=tf.nn.relu(Z2)\n",
    "    A2d=tf.nn.dropout(A2,keep_prob)\n",
    "    Z3=tf.add(tf.matmul(A2,parameters[\"W3\"]),parameters[\"b3\"])\n",
    "    A3=tf.nn.relu(Z3)\n",
    "    A3d=tf.nn.dropout(A3,keep_prob)\n",
    "    Z4=tf.add(tf.matmul(A3,parameters[\"W4\"]),parameters[\"b4\"])\n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z4 is Tensor(\"Add_3:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X,Y,keep_prob=placeholder(784,10,1)\n",
    "parameters=initialize_weights()\n",
    "Z4=feed_forward(X,parameters,1)\n",
    "print(\"Z4 is {}\".format(Z4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z4,Y):\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z4,labels=Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X,Y,keep_prob=placeholder(784,10,1)\n",
    "parameters=initialize_weights()\n",
    "Z4=feed_forward(X,parameters,1)\n",
    "cost=compute_cost(Z4,Y)\n",
    "print(\"Cost is {}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(X_train,Y_train,X_test,Y_test,number_of_epoch,mini_batch_size,learning_rate,dropout_prob,print_cost=True):\n",
    "    (m,n_x)=X_train.shape\n",
    "    (n_y)=Y_train.shape[1]\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    costs=[]\n",
    "    seed=3\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_random_seed(1)\n",
    "    \n",
    "    #First using the placeholders\n",
    "    X,Y,keep_prob=placeholder(n_x,n_y,dropout_prob)\n",
    "    \n",
    "    #Next initializing the weights\n",
    "    parameters=initialize_weights()\n",
    "    \n",
    "    #Next doing the first forward propagation\n",
    "    Z4=feed_forward(X,parameters,keep_prob)\n",
    "    \n",
    "    #Next Computing the cost\n",
    "    cost=compute_cost(Z4,Y)\n",
    "    \n",
    "    #The learning rate decay\n",
    "    lr=tf.train.exponential_decay(learning_rate,decay_rate=0.96,decay_steps=2000,global_step=global_step,staircase=True)\n",
    "    \n",
    "    #Which optimizer we will use\n",
    "    optimizer=tf.train.AdamOptimizer(beta1=0.98,beta2=0.999,epsilon=1e-08,learning_rate=lr,name=\"Adam\").minimize(cost,global_step)\\\n",
    "    \n",
    "    #the initializer which we will use\n",
    "    init=tf.global_variables_initializer()\n",
    "    \n",
    "    #Starting the session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        print(\"-x-x-x-x-x-x-x-x-x-x-x-x-x-INITIALIZED-x-x-x-x-x-x-x-x-x-x-x-x-x-x--x-x-x-x-x\")\n",
    "        num_of_mini_batches=math.floor(m/mini_batch_size)\n",
    "        print(\"The model will be trained for {} epochs and the number of mini batches are {}\".format(number_of_epoch,num_of_mini_batches))\n",
    "        for epoch in range(number_of_epoch):\n",
    "            epoch_cost=0\n",
    "            minibatches=random_mini_batches(X_train,Y_train,seed,mini_batch_size)\n",
    "            seed+=1\n",
    "            for minibatch in minibatches:\n",
    "                (X_minibatch,Y_minibatch)=minibatch\n",
    "                _,minibatchcost=sess.run([optimizer,cost],feed_dict={X:X_minibatch,Y:Y_minibatch,keep_prob:dropout_prob})\n",
    "                epoch_cost+=minibatchcost/num_of_mini_batches\n",
    "            #Adding the cost\n",
    "            if print_cost==True and epoch%100==0:\n",
    "                print(\"For the epoch {} the cost is {} with dropout probability of {}\".format(epoch,epoch_cost,dropout_prob))\n",
    "            if print_cost==True and epoch%5==0:\n",
    "                costs.append(epoch_cost)\n",
    "        print(\"-x-x-x-x-x-x-x-x-x-x-x-x-x MODEL TRAINED-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x\")\n",
    "        #printing the cost vs iteration curve\n",
    "        \n",
    "        print(\"the cost V/s Iterationn Curve Is -->\")\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.xlabel(\"Iterations in 10's\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.title(\"Cost for the learning rate {}\".format(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        #Saving the parameters\n",
    "        print(\"Saving the Parameters!\")\n",
    "        parameters=sess.run(parameters)\n",
    "        \n",
    "        #Now finding the predictions and accuracy\n",
    "        print(\"Calculating the accuracy--->\")\n",
    "        correct_predictions=tf.equal(tf.argmax(Z4,1),tf.argmax(Y,1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_predictions,\"float\"))\n",
    "        print(\"The training accuracy is {}\".format(accuracy.eval({X:X_train,Y:Y_train,keep_prob:dropout_prob})))\n",
    "        print(\"The testing accuracy is {}\".format(accuracy.eval({X:X_test,Y:Y_test,keep_prob:1.0})))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-x-x-x-x-x-x-x-x-x-x-x-x-x-INITIALIZED-x-x-x-x-x-x-x-x-x-x-x-x-x-x--x-x-x-x-x\n",
      "The model will be trained for 500 epochs and the number of mini batches are 937\n",
      "For the epoch 0 the cost is 3.749418838301485 with dropout probability of 1.0\n",
      "For the epoch 100 the cost is 2.306520904013799 with dropout probability of 1.0\n",
      "For the epoch 200 the cost is 2.3053317337178623 with dropout probability of 1.0\n",
      "For the epoch 300 the cost is 2.3050917054952933 with dropout probability of 1.0\n",
      "For the epoch 400 the cost is 2.305050681978302 with dropout probability of 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "parameters=neural_network_model(X_train,Y_train,X_test,Y_test,dropout_prob=1.0,learning_rate=0.1,mini_batch_size=64,number_of_epoch=500,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
